{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Personal Finance Expenditures Dataset\n",
    "# https://www.kaggle.com/datasets/bukolafatunde/personal-finance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pfdf = pd.read_csv(r'C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\personal_transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Transaction Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Account Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>11.11</td>\n",
       "      <td>debit</td>\n",
       "      <td>Shopping</td>\n",
       "      <td>Platinum Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/02/2018</td>\n",
       "      <td>Mortgage Payment</td>\n",
       "      <td>1247.44</td>\n",
       "      <td>debit</td>\n",
       "      <td>Mortgage &amp; Rent</td>\n",
       "      <td>Checking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/02/2018</td>\n",
       "      <td>Thai Restaurant</td>\n",
       "      <td>24.22</td>\n",
       "      <td>debit</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Silver Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/03/2018</td>\n",
       "      <td>Credit Card Payment</td>\n",
       "      <td>2298.09</td>\n",
       "      <td>credit</td>\n",
       "      <td>Credit Card Payment</td>\n",
       "      <td>Platinum Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/04/2018</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>11.76</td>\n",
       "      <td>debit</td>\n",
       "      <td>Movies &amp; DVDs</td>\n",
       "      <td>Platinum Card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date          Description   Amount Transaction Type  \\\n",
       "0  01/01/2018               Amazon    11.11            debit   \n",
       "1  01/02/2018     Mortgage Payment  1247.44            debit   \n",
       "2  01/02/2018      Thai Restaurant    24.22            debit   \n",
       "3  01/03/2018  Credit Card Payment  2298.09           credit   \n",
       "4  01/04/2018              Netflix    11.76            debit   \n",
       "\n",
       "              Category   Account Name  \n",
       "0             Shopping  Platinum Card  \n",
       "1      Mortgage & Rent       Checking  \n",
       "2          Restaurants    Silver Card  \n",
       "3  Credit Card Payment  Platinum Card  \n",
       "4        Movies & DVDs  Platinum Card  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(806, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleepdf = pd.read_csv(r'C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\sleep_health_and_lifestyle_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sleep Duration</th>\n",
       "      <th>Quality of Sleep</th>\n",
       "      <th>Physical Activity Level</th>\n",
       "      <th>Stress Level</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Daily Steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>374.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>374.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>187.500000</td>\n",
       "      <td>42.184492</td>\n",
       "      <td>7.132086</td>\n",
       "      <td>7.312834</td>\n",
       "      <td>59.171123</td>\n",
       "      <td>5.385027</td>\n",
       "      <td>70.165775</td>\n",
       "      <td>6816.844920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>108.108742</td>\n",
       "      <td>8.673133</td>\n",
       "      <td>0.795657</td>\n",
       "      <td>1.196956</td>\n",
       "      <td>20.830804</td>\n",
       "      <td>1.774526</td>\n",
       "      <td>4.135676</td>\n",
       "      <td>1617.915679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>94.250000</td>\n",
       "      <td>35.250000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>5600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>187.500000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>7000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>280.750000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>374.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Person ID         Age  Sleep Duration  Quality of Sleep  \\\n",
       "count  374.000000  374.000000      374.000000        374.000000   \n",
       "mean   187.500000   42.184492        7.132086          7.312834   \n",
       "std    108.108742    8.673133        0.795657          1.196956   \n",
       "min      1.000000   27.000000        5.800000          4.000000   \n",
       "25%     94.250000   35.250000        6.400000          6.000000   \n",
       "50%    187.500000   43.000000        7.200000          7.000000   \n",
       "75%    280.750000   50.000000        7.800000          8.000000   \n",
       "max    374.000000   59.000000        8.500000          9.000000   \n",
       "\n",
       "       Physical Activity Level  Stress Level  Heart Rate   Daily Steps  \n",
       "count               374.000000    374.000000  374.000000    374.000000  \n",
       "mean                 59.171123      5.385027   70.165775   6816.844920  \n",
       "std                  20.830804      1.774526    4.135676   1617.915679  \n",
       "min                  30.000000      3.000000   65.000000   3000.000000  \n",
       "25%                  45.000000      4.000000   68.000000   5600.000000  \n",
       "50%                  60.000000      5.000000   70.000000   7000.000000  \n",
       "75%                  75.000000      7.000000   72.000000   8000.000000  \n",
       "max                  90.000000      8.000000   86.000000  10000.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleepdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Person ID', 'Gender', 'Age', 'Occupation', 'Sleep Duration',\n",
      "       'Quality of Sleep', 'Physical Activity Level', 'Stress Level',\n",
      "       'BMI Category', 'Blood Pressure', 'Heart Rate', 'Daily Steps',\n",
      "       'Sleep Disorder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(sleepdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             None\n",
       "1             None\n",
       "2             None\n",
       "3      Sleep Apnea\n",
       "4      Sleep Apnea\n",
       "          ...     \n",
       "369    Sleep Apnea\n",
       "370    Sleep Apnea\n",
       "371    Sleep Apnea\n",
       "372    Sleep Apnea\n",
       "373    Sleep Apnea\n",
       "Name: Sleep Disorder, Length: 374, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleepdf['Sleep Disorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None           219\n",
      "Sleep Apnea     78\n",
      "Insomnia        77\n",
      "Name: Sleep Disorder, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the list of unique strings and their counts\n",
    "sleep_disorder_counts = sleepdf['Sleep Disorder'].value_counts()\n",
    "\n",
    "# Print the list and counts\n",
    "print(sleep_disorder_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Person ID                                                      \\\n",
      "                   count        mean         std  min     25%    50%     75%   \n",
      "Sleep Disorder                                                                 \n",
      "Insomnia            77.0  216.272727   65.982581  6.0  195.00  231.0  252.00   \n",
      "None               219.0  146.972603  100.646895  1.0   65.50  128.0  207.50   \n",
      "Sleep Apnea         78.0  272.884615  103.707378  4.0  270.25  292.5  352.75   \n",
      "\n",
      "                         Age             ... Heart Rate       Daily Steps  \\\n",
      "                  max  count       mean  ...        75%   max       count   \n",
      "Sleep Disorder                           ...                                \n",
      "Insomnia        316.0   77.0  43.519481  ...       72.0  85.0        77.0   \n",
      "None            360.0  219.0  39.036530  ...       70.0  77.0       219.0   \n",
      "Sleep Apnea     374.0   78.0  49.705128  ...       75.0  86.0        78.0   \n",
      "\n",
      "                                                                           \\\n",
      "                       mean          std     min     25%     50%      75%   \n",
      "Sleep Disorder                                                              \n",
      "Insomnia        5901.298701  1000.328039  3000.0  6000.0  6000.0   6000.0   \n",
      "None            6852.968037  1393.473600  4200.0  5000.0  7000.0   8000.0   \n",
      "Sleep Apnea     7619.230769  2168.191400  3000.0  7000.0  7000.0  10000.0   \n",
      "\n",
      "                         \n",
      "                    max  \n",
      "Sleep Disorder           \n",
      "Insomnia        10000.0  \n",
      "None            10000.0  \n",
      "Sleep Apnea     10000.0  \n",
      "\n",
      "[3 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group the dataframe by 'Sleep Disorder' and calculate summary statistics\n",
    "summary_stats = sleepdf.groupby('Sleep Disorder').describe()\n",
    "\n",
    "# Select the desired statistics (e.g., count, mean, std, min, max)\n",
    "# summary_stats = summary_stats.loc[['None', 'Sleep Apnea', 'Insomnia'], ['count', 'mean', 'std', 'min', 'max']]\n",
    "\n",
    "# Print the summary statistics\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_34408\\2900760375.py:1: DtypeWarning: Columns (9,1537,1540,1542,1606,1608,1614,1615,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1703,1704,1725,1726,1727,1728,1729,1743,1815,1816,1817,1818,1823,1824,1830,1831,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1909,1910,1911,1912,1913,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1983,1984,2376,2377,2403,2404,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2958) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  recentinst_df = pd.read_csv(r'C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\US College Scorecard\\Most-Recent-Cohorts-Institution.csv')\n"
     ]
    }
   ],
   "source": [
    "recentinst_df = pd.read_csv(r'C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\US College Scorecard\\Most-Recent-Cohorts-Institution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "recentfos_df = pd.read_csv(r'C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\US College Scorecard\\Most-Recent-Cohorts-Field-of-Study.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PLUS_DEBT_INST_COMP_MD'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\thisone\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\thisone\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\thisone\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PLUS_DEBT_INST_COMP_MD'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m recentfos_df[\u001b[39m'\u001b[39m\u001b[39mDEBT_ALL_STGP_EVAL_MDN\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m recentfos_df[\u001b[39m'\u001b[39m\u001b[39mDEBT_ALL_STGP_ANY_MDN\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m recentfos_df[\u001b[39m'\u001b[39;49m\u001b[39mPLUS_DEBT_INST_COMP_MD\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\thisone\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\thisone\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PLUS_DEBT_INST_COMP_MD'"
     ]
    }
   ],
   "source": [
    "# Recent Cohort Field of Study Debt Variables, all 'PrivacySuppressed'\n",
    "recentfos_df['DEBT_ALL_STGP_EVAL_MDN10YRPAY']\n",
    "recentfos_df['DEBT_ALL_STGP_ANY_MDN10YRPAY']\n",
    "recentfos_df['DEBT_ALL_STGP_EVAL_MDN']\n",
    "recentfos_df['DEBT_ALL_STGP_ANY_MDN']\n",
    "recentfos_df['PLUS_DEBT_INST_COMP_MD_SUPP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNITID                       11680\n",
      "OPEID6                           0\n",
      "INSTNM                           0\n",
      "CONTROL                          0\n",
      "MAIN                             0\n",
      "                             ...  \n",
      "BBRR4_FED_COMP_NOPROG            0\n",
      "BBRR4_FED_COMP_MAKEPROG          0\n",
      "BBRR4_FED_COMP_PAIDINFULL        0\n",
      "BBRR4_FED_COMP_DISCHARGE         0\n",
      "DISTANCE                         0\n",
      "Length: 160, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Values\n",
    "# Count the nulls in each column\n",
    "null_counts = recentfos_df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['UNITID', 'OPEID6', 'INSTNM', 'CONTROL', 'MAIN', 'CIPCODE', 'CIPDESC',\n",
      "       'CREDLEV', 'CREDDESC', 'IPEDSCOUNT1',\n",
      "       ...\n",
      "       'BBRR4_FED_COMP_N', 'BBRR4_FED_COMP_DFLT', 'BBRR4_FED_COMP_DLNQ',\n",
      "       'BBRR4_FED_COMP_FBR', 'BBRR4_FED_COMP_DFR', 'BBRR4_FED_COMP_NOPROG',\n",
      "       'BBRR4_FED_COMP_MAKEPROG', 'BBRR4_FED_COMP_PAIDINFULL',\n",
      "       'BBRR4_FED_COMP_DISCHARGE', 'DISTANCE'],\n",
      "      dtype='object', length=160)\n"
     ]
    }
   ],
   "source": [
    "# recentfos_df['OPEFLAG']\n",
    "print(recentfos_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         PrivacySuppressed\n",
       "1         PrivacySuppressed\n",
       "2         PrivacySuppressed\n",
       "3         PrivacySuppressed\n",
       "4         PrivacySuppressed\n",
       "                ...        \n",
       "233974    PrivacySuppressed\n",
       "233975    PrivacySuppressed\n",
       "233976    PrivacySuppressed\n",
       "233977    PrivacySuppressed\n",
       "233978    PrivacySuppressed\n",
       "Name: BBRR4_FED_COMP_DFLT, Length: 233979, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recentfos_df['BBRR4_FED_COMP_DFLT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['UNITID', 'OPEID', 'OPEID6', 'INSTNM', 'CITY', 'STABBR', 'ZIP',\n",
      "       'ACCREDAGENCY', 'INSTURL', 'NPCURL',\n",
      "       ...\n",
      "       'BBRR4_PP_MALE_DISCHARGE', 'BBRR4_PP_NOMALE_N', 'BBRR4_PP_NOMALE_DFLT',\n",
      "       'BBRR4_PP_NOMALE_DLNQ', 'BBRR4_PP_NOMALE_FBR', 'BBRR4_PP_NOMALE_DFR',\n",
      "       'BBRR4_PP_NOMALE_NOPROG', 'BBRR4_PP_NOMALE_MAKEPROG',\n",
      "       'BBRR4_PP_NOMALE_PAIDINFULL', 'BBRR4_PP_NOMALE_DISCHARGE'],\n",
      "      dtype='object', length=3214)\n"
     ]
    }
   ],
   "source": [
    "print(recentinst_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               0.05-0.09\n",
       "1                  <=0.10\n",
       "2       PrivacySuppressed\n",
       "3                  <=0.10\n",
       "4               0.05-0.09\n",
       "              ...        \n",
       "6538                 0.03\n",
       "6539                 0.03\n",
       "6540                 0.03\n",
       "6541                 0.03\n",
       "6542                  NaN\n",
       "Name: BBRR4_PP_MALE_DISCHARGE, Length: 6543, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recentinst_df['BBRR4_PP_MALE_DISCHARGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNITID                           0\n",
       "OPEID                            0\n",
       "OPEID6                           0\n",
       "INSTNM                           0\n",
       "CITY                             0\n",
       "                              ... \n",
       "BBRR4_PP_NOMALE_DFR           4821\n",
       "BBRR4_PP_NOMALE_NOPROG        4822\n",
       "BBRR4_PP_NOMALE_MAKEPROG      4820\n",
       "BBRR4_PP_NOMALE_PAIDINFULL    4705\n",
       "BBRR4_PP_NOMALE_DISCHARGE     4822\n",
       "Length: 3214, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PrivacySuppressed Counts in Recent Field of Study\n",
    "PS_counts = (recentinst_df == \"PrivacySuppressed\").sum()\n",
    "PS_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNITID                           0\n",
       "OPEID                            0\n",
       "OPEID6                           0\n",
       "INSTNM                           0\n",
       "CITY                             0\n",
       "                              ... \n",
       "BBRR4_PP_NOMALE_DFR           4821\n",
       "BBRR4_PP_NOMALE_NOPROG        4822\n",
       "BBRR4_PP_NOMALE_MAKEPROG      4820\n",
       "BBRR4_PP_NOMALE_PAIDINFULL    4705\n",
       "BBRR4_PP_NOMALE_DISCHARGE     4822\n",
       "Length: 3214, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PSinst_counts = (recentinst_df == 'PrivacySuppressed').sum()\n",
    "PSinst_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          <=0.05\n",
      "1          <=0.10\n",
      "3          <=0.10\n",
      "4       0.05-0.09\n",
      "5       0.09-0.10\n",
      "          ...    \n",
      "6538         0.07\n",
      "6539         0.07\n",
      "6540         0.07\n",
      "6541         0.07\n",
      "6542          NaN\n",
      "Name: BBRR4_PP_NOMALE_NOPROG, Length: 1721, dtype: object\n"
     ]
    }
   ],
   "source": [
    "filtered_values = recentinst_df[recentinst_df['BBRR4_PP_NOMALE_NOPROG'] != 'PrivacySuppressed']['BBRR4_PP_NOMALE_NOPROG']\n",
    "print(filtered_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Credit Card Approval\n",
    "targetcc = pd.read_csv(r'C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\credit_card_approval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>NAME_HOUSING_TYPE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>JOB</th>\n",
       "      <th>BEGIN_MONTHS</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5065438</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>2+ children</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>With parents</td>\n",
       "      <td>-13258</td>\n",
       "      <td>-2300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Managers</td>\n",
       "      <td>-6</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5142753</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>No children</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-17876</td>\n",
       "      <td>-377</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Private service staff</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5111146</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>No children</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-19579</td>\n",
       "      <td>-1028</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5010310</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1 children</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-15109</td>\n",
       "      <td>-1956</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5010835</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>2+ children</td>\n",
       "      <td>139500.0</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-17281</td>\n",
       "      <td>-5578</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Drivers</td>\n",
       "      <td>-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY CNT_CHILDREN  \\\n",
       "0  5065438           F            Y               N  2+ children   \n",
       "1  5142753           F            N               N  No children   \n",
       "2  5111146           M            Y               Y  No children   \n",
       "3  5010310           F            Y               Y   1 children   \n",
       "4  5010835           M            Y               Y  2+ children   \n",
       "\n",
       "   AMT_INCOME_TOTAL            NAME_EDUCATION_TYPE    NAME_FAMILY_STATUS  \\\n",
       "0          270000.0  Secondary / secondary special               Married   \n",
       "1           81000.0  Secondary / secondary special  Single / not married   \n",
       "2          270000.0               Higher education               Married   \n",
       "3          112500.0  Secondary / secondary special               Married   \n",
       "4          139500.0  Secondary / secondary special               Married   \n",
       "\n",
       "   NAME_HOUSING_TYPE  DAYS_BIRTH  DAYS_EMPLOYED  FLAG_MOBIL  FLAG_WORK_PHONE  \\\n",
       "0       With parents      -13258          -2300           1                0   \n",
       "1  House / apartment      -17876           -377           1                1   \n",
       "2  House / apartment      -19579          -1028           1                0   \n",
       "3  House / apartment      -15109          -1956           1                0   \n",
       "4  House / apartment      -17281          -5578           1                1   \n",
       "\n",
       "   FLAG_PHONE  FLAG_EMAIL                    JOB  BEGIN_MONTHS STATUS  TARGET  \n",
       "0           0           0               Managers            -6      C       0  \n",
       "1           1           0  Private service staff            -4      0       0  \n",
       "2           1           0               Laborers             0      C       0  \n",
       "3           0           0             Core staff            -3      0       0  \n",
       "4           0           0                Drivers           -29      0       0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetcc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf0 in position 5938: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m      4\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCareer\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLighthouseLabs\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFinal Project\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mPotential Datasets\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFDMQ_TOTAL_Englishversion.sav\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m df, meta \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(file)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\thisone\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[0;32m    323\u001b[0m     \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf0 in position 5938: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "file = open('C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\FDMQ_TOTAL_Englishversion.sav')\n",
    "df, meta = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'fdmq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyreadstat\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpyr\u001b[39;00m\n\u001b[0;32m      3\u001b[0m fdmq \u001b[39m=\u001b[39m pyr\u001b[39m.\u001b[39mread_sav(\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCareer\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLighthouseLabs\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFinal Project\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mPotential Datasets\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFDMQ_TOTAL_Englishversion.sav\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m fdmq \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mfdmq\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\thisone\\lib\\site-packages\\pandas\\__init__.py:264\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m SparseArray \u001b[39mas\u001b[39;00m _SparseArray\n\u001b[0;32m    262\u001b[0m     \u001b[39mreturn\u001b[39;00m _SparseArray\n\u001b[1;32m--> 264\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpandas\u001b[39m\u001b[39m'\u001b[39m\u001b[39m has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'fdmq'"
     ]
    }
   ],
   "source": [
    "# FDMQ\n",
    "import pyreadstat as pyr\n",
    "fdmq = pyr.read_sav('C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\FDMQ_TOTAL_Englishversion.sav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Recruitment  Inclusion_groups  ADHD_4groups  ADHD_4groups_AvsN  \\\n",
      "0             1.0               1.0           0.0                0.0   \n",
      "1             1.0               1.0           1.0                NaN   \n",
      "2             1.0               0.0           0.0                0.0   \n",
      "3             1.0               0.0           0.0                0.0   \n",
      "4             2.0               0.0           0.0                0.0   \n",
      "...           ...               ...           ...                ...   \n",
      "1287          1.0               1.0           0.0                0.0   \n",
      "1288          1.0               1.0           0.0                0.0   \n",
      "1289          1.0               1.0           0.0                0.0   \n",
      "1290          2.0               1.0           0.0                0.0   \n",
      "1291          1.0               0.0           NaN                NaN   \n",
      "\n",
      "      ADHD_4groups_AvsS  ADHD_4groups_AvsAO  ADHD_4groups_AOvsS  \\\n",
      "0                   NaN                 NaN                 NaN   \n",
      "1                   1.0                 NaN                 1.0   \n",
      "2                   NaN                 NaN                 NaN   \n",
      "3                   NaN                 NaN                 NaN   \n",
      "4                   NaN                 NaN                 NaN   \n",
      "...                 ...                 ...                 ...   \n",
      "1287                NaN                 NaN                 NaN   \n",
      "1288                NaN                 NaN                 NaN   \n",
      "1289                NaN                 NaN                 NaN   \n",
      "1290                NaN                 NaN                 NaN   \n",
      "1291                NaN                 NaN                 NaN   \n",
      "\n",
      "      ADHD_4groups_AOvsN  ADHD_4groups_SvsN   Age  ...  \\\n",
      "0                    0.0                0.0   NaN  ...   \n",
      "1                    NaN                1.0   NaN  ...   \n",
      "2                    0.0                0.0   NaN  ...   \n",
      "3                    0.0                0.0   NaN  ...   \n",
      "4                    0.0                0.0   NaN  ...   \n",
      "...                  ...                ...   ...  ...   \n",
      "1287                 0.0                0.0  18.0  ...   \n",
      "1288                 0.0                0.0  18.0  ...   \n",
      "1289                 0.0                0.0  18.0  ...   \n",
      "1290                 0.0                0.0  18.0  ...   \n",
      "1291                 NaN                NaN  18.0  ...   \n",
      "\n",
      "      ADHD_retrospective_presentation  ADHD_current_presentation  \\\n",
      "0                                 0.0                        0.0   \n",
      "1                                 0.0                        0.0   \n",
      "2                                 0.0                        0.0   \n",
      "3                                 0.0                        0.0   \n",
      "4                                 0.0                        0.0   \n",
      "...                               ...                        ...   \n",
      "1287                              0.0                        0.0   \n",
      "1288                              0.0                        0.0   \n",
      "1289                              0.0                        0.0   \n",
      "1290                              0.0                        0.0   \n",
      "1291                              0.0                        0.0   \n",
      "\n",
      "      BRIEFA_inconsistency  BRIEFA_infrequency  BRIEFA_negativity  \\\n",
      "0                      NaN                 NaN                NaN   \n",
      "1                      NaN                 NaN                NaN   \n",
      "2                      NaN                 NaN                NaN   \n",
      "3                      NaN                 NaN                NaN   \n",
      "4                      0.0                 0.0                0.0   \n",
      "...                    ...                 ...                ...   \n",
      "1287                   NaN                 NaN                NaN   \n",
      "1288                   NaN                 NaN                NaN   \n",
      "1289                   NaN                 NaN                NaN   \n",
      "1290                   0.0                 0.0                0.0   \n",
      "1291                   NaN                 NaN                NaN   \n",
      "\n",
      "      AdministrationTime  Stimulants  Antidepressants  Benzo  Other  \n",
      "0                   22.0         0.0              0.0    0.0    1.0  \n",
      "1                15136.0         0.0              0.0    0.0    1.0  \n",
      "2                   70.0         0.0              0.0    0.0    1.0  \n",
      "3                   29.0         0.0              0.0    0.0    0.0  \n",
      "4                   78.0         0.0              1.0    1.0    1.0  \n",
      "...                  ...         ...              ...    ...    ...  \n",
      "1287                52.0         0.0              0.0    0.0    0.0  \n",
      "1288                22.0         0.0              0.0    0.0    0.0  \n",
      "1289                72.0         0.0              0.0    0.0    1.0  \n",
      "1290                14.0         0.0              0.0    0.0    0.0  \n",
      "1291                47.0         0.0              0.0    0.0    1.0  \n",
      "\n",
      "[1292 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'data' element of the tuple into a DataFrame\n",
    "fdmq_df = pd.DataFrame(fdmq[0])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(fdmq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdmq_df.to_csv('C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\FDMQ_TOTAL_Englishversion.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recruitment</th>\n",
       "      <th>Inclusion_groups</th>\n",
       "      <th>ADHD_4groups</th>\n",
       "      <th>ADHD_4groups_AvsN</th>\n",
       "      <th>ADHD_4groups_AvsS</th>\n",
       "      <th>ADHD_4groups_AvsAO</th>\n",
       "      <th>ADHD_4groups_AOvsS</th>\n",
       "      <th>ADHD_4groups_AOvsN</th>\n",
       "      <th>ADHD_4groups_SvsN</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>ADHD_retrospective_presentation</th>\n",
       "      <th>ADHD_current_presentation</th>\n",
       "      <th>BRIEFA_inconsistency</th>\n",
       "      <th>BRIEFA_infrequency</th>\n",
       "      <th>BRIEFA_negativity</th>\n",
       "      <th>AdministrationTime</th>\n",
       "      <th>Stimulants</th>\n",
       "      <th>Antidepressants</th>\n",
       "      <th>Benzo</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1292.000000</td>\n",
       "      <td>1292.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>935.000000</td>\n",
       "      <td>1040.000000</td>\n",
       "      <td>1268.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1292.000000</td>\n",
       "      <td>1292.000000</td>\n",
       "      <td>468.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>1292.000000</td>\n",
       "      <td>1292.000000</td>\n",
       "      <td>1292.000000</td>\n",
       "      <td>1292.000000</td>\n",
       "      <td>1292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.366873</td>\n",
       "      <td>0.410217</td>\n",
       "      <td>0.362205</td>\n",
       "      <td>0.149351</td>\n",
       "      <td>1.442308</td>\n",
       "      <td>2.446602</td>\n",
       "      <td>1.260274</td>\n",
       "      <td>0.121925</td>\n",
       "      <td>0.155769</td>\n",
       "      <td>48.082019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207430</td>\n",
       "      <td>0.146285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>258.313467</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.043344</td>\n",
       "      <td>0.016254</td>\n",
       "      <td>0.272446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482138</td>\n",
       "      <td>0.492063</td>\n",
       "      <td>0.756783</td>\n",
       "      <td>0.652846</td>\n",
       "      <td>0.832050</td>\n",
       "      <td>0.499571</td>\n",
       "      <td>0.439789</td>\n",
       "      <td>0.478779</td>\n",
       "      <td>0.362811</td>\n",
       "      <td>17.964612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672814</td>\n",
       "      <td>0.546344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1326.295157</td>\n",
       "      <td>0.055577</td>\n",
       "      <td>0.203708</td>\n",
       "      <td>0.126499</td>\n",
       "      <td>0.445390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15136.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Recruitment  Inclusion_groups  ADHD_4groups  ADHD_4groups_AvsN  \\\n",
       "count  1292.000000       1292.000000   1143.000000         924.000000   \n",
       "mean      1.366873          0.410217      0.362205           0.149351   \n",
       "std       0.482138          0.492063      0.756783           0.652846   \n",
       "min       1.000000          0.000000      0.000000           0.000000   \n",
       "25%       1.000000          0.000000      0.000000           0.000000   \n",
       "50%       1.000000          0.000000      0.000000           0.000000   \n",
       "75%       2.000000          1.000000      0.000000           0.000000   \n",
       "max       2.000000          1.000000      3.000000           3.000000   \n",
       "\n",
       "       ADHD_4groups_AvsS  ADHD_4groups_AvsAO  ADHD_4groups_AOvsS  \\\n",
       "count         208.000000          103.000000          219.000000   \n",
       "mean            1.442308            2.446602            1.260274   \n",
       "std             0.832050            0.499571            0.439789   \n",
       "min             1.000000            2.000000            1.000000   \n",
       "25%             1.000000            2.000000            1.000000   \n",
       "50%             1.000000            2.000000            1.000000   \n",
       "75%             1.000000            3.000000            2.000000   \n",
       "max             3.000000            3.000000            2.000000   \n",
       "\n",
       "       ADHD_4groups_AOvsN  ADHD_4groups_SvsN          Age  ...  \\\n",
       "count          935.000000        1040.000000  1268.000000  ...   \n",
       "mean             0.121925           0.155769    48.082019  ...   \n",
       "std              0.478779           0.362811    17.964612  ...   \n",
       "min              0.000000           0.000000    18.000000  ...   \n",
       "25%              0.000000           0.000000    32.000000  ...   \n",
       "50%              0.000000           0.000000    50.000000  ...   \n",
       "75%              0.000000           0.000000    64.000000  ...   \n",
       "max              2.000000           1.000000    80.000000  ...   \n",
       "\n",
       "       ADHD_retrospective_presentation  ADHD_current_presentation  \\\n",
       "count                      1292.000000                1292.000000   \n",
       "mean                          0.207430                   0.146285   \n",
       "std                           0.672814                   0.546344   \n",
       "min                           0.000000                   0.000000   \n",
       "25%                           0.000000                   0.000000   \n",
       "50%                           0.000000                   0.000000   \n",
       "75%                           0.000000                   0.000000   \n",
       "max                           3.000000                   3.000000   \n",
       "\n",
       "       BRIEFA_inconsistency  BRIEFA_infrequency  BRIEFA_negativity  \\\n",
       "count                 468.0               468.0              468.0   \n",
       "mean                    0.0                 0.0                0.0   \n",
       "std                     0.0                 0.0                0.0   \n",
       "min                     0.0                 0.0                0.0   \n",
       "25%                     0.0                 0.0                0.0   \n",
       "50%                     0.0                 0.0                0.0   \n",
       "75%                     0.0                 0.0                0.0   \n",
       "max                     0.0                 0.0                0.0   \n",
       "\n",
       "       AdministrationTime   Stimulants  Antidepressants        Benzo  \\\n",
       "count         1292.000000  1292.000000      1292.000000  1292.000000   \n",
       "mean           258.313467     0.003096         0.043344     0.016254   \n",
       "std           1326.295157     0.055577         0.203708     0.126499   \n",
       "min              5.000000     0.000000         0.000000     0.000000   \n",
       "25%             25.000000     0.000000         0.000000     0.000000   \n",
       "50%             34.000000     0.000000         0.000000     0.000000   \n",
       "75%             49.000000     0.000000         0.000000     0.000000   \n",
       "max          15136.000000     1.000000         1.000000     1.000000   \n",
       "\n",
       "             Other  \n",
       "count  1292.000000  \n",
       "mean      0.272446  \n",
       "std       0.445390  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 79 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdmq_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>0.262949</td>\n",
       "      <td>0.139376</td>\n",
       "      <td>-1134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>...</td>\n",
       "      <td>School</td>\n",
       "      <td>0.622246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-828.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Government</td>\n",
       "      <td>0.555912</td>\n",
       "      <td>0.729567</td>\n",
       "      <td>-815.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>0.650442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-617.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Religion</td>\n",
       "      <td>0.322738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   ...       ORGANIZATION_TYPE EXT_SOURCE_2 EXT_SOURCE_3  \\\n",
       "0  ...  Business Entity Type 3     0.262949     0.139376   \n",
       "1  ...                  School     0.622246          NaN   \n",
       "2  ...              Government     0.555912     0.729567   \n",
       "3  ...  Business Entity Type 3     0.650442          NaN   \n",
       "4  ...                Religion     0.322738          NaN   \n",
       "\n",
       "  DAYS_LAST_PHONE_CHANGE AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                -1134.0                        0.0                       0.0   \n",
       "1                 -828.0                        0.0                       0.0   \n",
       "2                 -815.0                        0.0                       0.0   \n",
       "3                 -617.0                        NaN                       NaN   \n",
       "4                -1106.0                        0.0                       0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loandf = pd.read_csv(r'C:\\Career\\LighthouseLabs\\Midterm Partner Project_Week7\\application_data_clean.csv')\n",
    "loandf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADHD Reddit Women\n",
    "import pandas as pd\n",
    "adhdw = pd.read_csv(r'C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\ADHD Reddit\\adhdwomen-comment.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'd like to see this sub be more active, too. ...</td>\n",
       "      <td>cqowxhs</td>\n",
       "      <td>1</td>\n",
       "      <td>1430023102</td>\n",
       "      <td>2015-04-26 04:38:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've found people are more receptive when you ...</td>\n",
       "      <td>cvzg3v2</td>\n",
       "      <td>1</td>\n",
       "      <td>1444835103</td>\n",
       "      <td>2015-10-14 15:05:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you so much. I have been trying to use m...</td>\n",
       "      <td>cw65vo8</td>\n",
       "      <td>1</td>\n",
       "      <td>1445326215</td>\n",
       "      <td>2015-10-20 07:30:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>d2tscyn</td>\n",
       "      <td>1</td>\n",
       "      <td>1462457040</td>\n",
       "      <td>2016-05-05 14:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sooooo, not sure why you were told it was 24 h...</td>\n",
       "      <td>d38enqz</td>\n",
       "      <td>1</td>\n",
       "      <td>1463457224</td>\n",
       "      <td>2016-05-17 03:53:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body       id  score  \\\n",
       "0  I'd like to see this sub be more active, too. ...  cqowxhs      1   \n",
       "1  I've found people are more receptive when you ...  cvzg3v2      1   \n",
       "2  Thank you so much. I have been trying to use m...  cw65vo8      1   \n",
       "3                                          [deleted]  d2tscyn      1   \n",
       "4  Sooooo, not sure why you were told it was 24 h...  d38enqz      1   \n",
       "\n",
       "   created_utc     created_datetime  \n",
       "0   1430023102  2015-04-26 04:38:22  \n",
       "1   1444835103  2015-10-14 15:05:03  \n",
       "2   1445326215  2015-10-20 07:30:15  \n",
       "3   1462457040  2016-05-05 14:04:00  \n",
       "4   1463457224  2016-05-17 03:53:44  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adhdw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you so much. I have been trying to use my phone to set alerts and reminders all the time. I have a large support group but sometimes things slip out of their mouths and I just try to ignore it until I have time alone and then it hits me'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " adhdw['body'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>202658.000000</td>\n",
       "      <td>2.026580e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.064143</td>\n",
       "      <td>1.600717e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.352811</td>\n",
       "      <td>1.587645e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-31.000000</td>\n",
       "      <td>1.430023e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.594345e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.604373e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.611958e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>556.000000</td>\n",
       "      <td>1.620092e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               score   created_utc\n",
       "count  202658.000000  2.026580e+05\n",
       "mean        2.064143  1.600717e+09\n",
       "std         5.352811  1.587645e+07\n",
       "min       -31.000000  1.430023e+09\n",
       "25%         1.000000  1.594345e+09\n",
       "50%         1.000000  1.604373e+09\n",
       "75%         2.000000  1.611958e+09\n",
       "max       556.000000  1.620092e+09"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   adhdw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2480      Like, outside of context you may be right. But...\n",
      "20617     Actually lots of folks failed at college becau...\n",
      "20720     I did full psycho educational testing as part ...\n",
      "52103     There are people way more fucked up in the hea...\n",
      "52129     That psychiatrist is full of shit. I'm so sorr...\n",
      "                                ...                        \n",
      "202236    I have a very sensitive gag reflex, so my spou...\n",
      "202255    I bought a magnetic whiteboard for the front o...\n",
      "202265    I read somewhere that if plastic containers ha...\n",
      "202632    The book is called\\n\\n\\n You Mean I'm Not Lazy...\n",
      "202646    Ugh, I have the other part - where the 'gifted...\n",
      "Name: body, Length: 338, dtype: object\n"
     ]
    }
   ],
   "source": [
    "high_scoresw = adhdw.loc[adhdw['score'] > 50, 'body']\n",
    "print(high_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(338,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_scoresw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770                         How you can say this is a scam?\n",
      "799       Do you think this life has certain expectation...\n",
      "2139      I know this might be really unpopular here, bu...\n",
      "2491      I dont mean sub as less than... I was thinkin...\n",
      "2495      Dont you ever wonder why though... I mean the...\n",
      "                                ...                        \n",
      "202022    If you can afford it, I'd recommend getting yo...\n",
      "202032    Something you can try that has helped me (my h...\n",
      "202109    Well, that's your opinion of course, and you a...\n",
      "202118    You stated: \"small amounts of alcohol is terri...\n",
      "202319    Sorry, I don't buy this at all. It's unbelieva...\n",
      "Name: body, Length: 97, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Low Scores (downvoted) Women\n",
    "low_scoresw = adhdw.loc[adhdw['score'] < -1, 'body']\n",
    "print(low_scoresw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169360    This resonated with me a lot.\\n\\nI've always b...\n",
      "154488    They're using it to work 12 hours straight, me...\n",
      "140715    Are you bloody joking me? Normal people can re...\n",
      "168304    Ok but seriously, that asshole Duolingo owl is...\n",
      "167307    Forced prioritizing! I also thrive best when m...\n",
      "76082     Without electronic stimulation or diversion I ...\n",
      "146611    Lol I didn't know other people did this. I pro...\n",
      "169413    &gt; \\n&gt; \\n&gt; \\n&gt; \\n&gt; And then I ca...\n",
      "128598    Binge eating as a means of dopamine seeking is...\n",
      "140727    My neurotypical partner says no, so Im really...\n",
      "Name: body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Highest Scores for Women\n",
    "highest_scoresw = adhdw.nlargest(10, 'score')\n",
    "body_valuesw = highest_scoresw['body']\n",
    "print(body_valuesw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This resonated with me a lot.\\n\\nI've always been high-performing. What clued me in to the fact that I might have ADHD was the DBT that I was already going through to manage anxiety...because the only skill that seemed to work for me was sensory grounding, which is when I realized that some of my more problematic/embarrassing behaviors were not childish habits, they were stims.\\n\\nAnd then I came to realize that I was not having panic attacks, I was having meltdowns and shutdowns. Never at work or school, for some reason, but at weddings, and the zoo, and grad parties, and church, and under the table in my apartment.\\n\\nAnd, even though I could get work done, hyperfocus prevented me from taking care of my body at all to the point that I developed secondary disorders because of it. I ruined by digestion and possibly also my cardiac health.\\n\\nI was too rejection-sensitive to go for regular doctor's visits or make phone calls or go to the grocery store by myself, as a grown woman. \\n\\nBut my therapist always said, as long as I got to class on time, I didn't have a problem. Because who cares whether or not I was eating every day? I was on time for class! I was on the Dean's List! I was in the emergency room....\\n\\nBut, nope. Not a problem. Not as long as I got out of the ER at two in the morning in time for my biology exam at 7:30 AM.\\n\\nBecause productivity was the only thing that mattered.\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adhdw['body'][169360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADHD Reddit All\n",
    "adhd = pd.read_csv(r'C:\\Career\\LighthouseLabs\\Final Project\\Potential Datasets\\ADHD Reddit\\adhd-comment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>c08otkh</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.239042e+09</td>\n",
       "      <td>2009-04-06 18:18:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If I try to look this up right now I will get ...</td>\n",
       "      <td>c09y8qz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.243790e+09</td>\n",
       "      <td>2009-05-31 17:08:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>potassium is used as the thing that stops your...</td>\n",
       "      <td>c09yia6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.243815e+09</td>\n",
       "      <td>2009-06-01 00:07:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've love a link to anything about this.  \\n\\n...</td>\n",
       "      <td>c0a81e6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.244752e+09</td>\n",
       "      <td>2009-06-11 20:25:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't know anything specific, but I would *d...</td>\n",
       "      <td>c0aixrg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.245813e+09</td>\n",
       "      <td>2009-06-24 03:04:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body       id  score  \\\n",
       "0                                          [deleted]  c08otkh    1.0   \n",
       "1  If I try to look this up right now I will get ...  c09y8qz    2.0   \n",
       "2  potassium is used as the thing that stops your...  c09yia6    2.0   \n",
       "3  I've love a link to anything about this.  \\n\\n...  c0a81e6    3.0   \n",
       "4  I don't know anything specific, but I would *d...  c0aixrg    2.0   \n",
       "\n",
       "    created_utc     created_datetime  \n",
       "0  1.239042e+09  2009-04-06 18:18:07  \n",
       "1  1.243790e+09  2009-05-31 17:08:19  \n",
       "2  1.243815e+09  2009-06-01 00:07:50  \n",
       "3  1.244752e+09  2009-06-11 20:25:36  \n",
       "4  1.245813e+09  2009-06-24 03:04:51  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adhd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.356541e+06</td>\n",
       "      <td>3.356541e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.517950e+00</td>\n",
       "      <td>1.523066e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.003889e+01</td>\n",
       "      <td>7.274225e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.580000e+02</td>\n",
       "      <td>1.239042e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.466489e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.542173e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.585078e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.145000e+03</td>\n",
       "      <td>1.620086e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score   created_utc\n",
       "count  3.356541e+06  3.356541e+06\n",
       "mean   2.517950e+00  1.523066e+09\n",
       "std    1.003889e+01  7.274225e+07\n",
       "min   -1.580000e+02  1.239042e+09\n",
       "25%    1.000000e+00  1.466489e+09\n",
       "50%    1.000000e+00  1.542173e+09\n",
       "75%    2.000000e+00  1.585078e+09\n",
       "max    4.145000e+03  1.620086e+09"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adhd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57445      That other people who say \"oh, I've got ADHD\" ...\n",
      "70511      Welcome, brother. One point though. You did no...\n",
      "72801      Oh god, do I know that feeling. Gotta do this,...\n",
      "86560      **Masking:**\\n\\nOne of the biggest differences...\n",
      "87219      The single most painful memory I have of my sc...\n",
      "                                 ...                        \n",
      "3355961    Everyone thinks we should just be wild.  Nah...\n",
      "3355976    I agree! For me the worst part is when I expla...\n",
      "3356077    The bad first impressions are getting to me. I...\n",
      "3356423    Or you scream and cry and breakdown for help a...\n",
      "3356534    That is the worst part. Ill have weeks when m...\n",
      "Name: body, Length: 3448, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# High Scores All\n",
    "high_scores = adhd.loc[adhd['score'] > 100, 'body']\n",
    "print(high_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3448,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19659      I have ADHD and have a 3.8 - opt to take no me...\n",
      "28432                                              [deleted]\n",
      "37682      As someone who thinks ADHD is a faux condition...\n",
      "38695      Yeah, I would not trust it.  If you want advic...\n",
      "38697                Thats an ignorant comment, no offense. \n",
      "                                 ...                        \n",
      "3352246    I think thats just more of you seeking to con...\n",
      "3352289    My personal theory is that ADHD is related to ...\n",
      "3352353    People who truly have ADHD like myself can't w...\n",
      "3352356    Martins-Silva et al., (2019) conducted a line...\n",
      "3354988    fr. I see so many on this sub getting off on s...\n",
      "Name: body, Length: 678, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Low Scores (downvoted) All\n",
    "low_scores = adhd.loc[adhd['score'] < -10, 'body']\n",
    "print(low_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(678,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699023    Sometimes people might gently scold you becaus...\n",
      "3142654    Yeah you don't have ADHD, you just have *all t...\n",
      "3232519    My psych likened it to needing glasses. Someti...\n",
      "2838736    Question: How do you get an Adhd person to cle...\n",
      "3226920    If you havent already tried it, something tha...\n",
      "101930     This is kind of my specialty!  Beware! TL;DR a...\n",
      "2968428    The ole ADHD tax. You are right, and theres m...\n",
      "2087555    Ah yes ADHD: the superpower where you cant go...\n",
      "1769679    Either he admits that he has a substance abuse...\n",
      "3165314    ...so that's why when I was in college every t...\n",
      "Name: body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Highest Scores for All\n",
    "highest_scores = adhd.nlargest(10, 'score')\n",
    "body_values = highest_scores['body']\n",
    "print(body_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used these findings to call up entire single posts and read them, to get a feel for what gets posted generally and what kinds of posts attract the kind of attention and views that lead to high upvotes or downvotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3503.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adhd['score'][3142654]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gensim.models.word2vec' has no attribute 'load_word2vec_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mword2vec\u001b[39m.\u001b[39;49mload_word2vec_format(\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCareer\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLighthouseLabs\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDataScience_Week10\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mGoogleNews-vectors-negative300.bin.gz\u001b[39m\u001b[39m'\u001b[39m, binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gensim.models.word2vec' has no attribute 'load_word2vec_format'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# model = gensim.models.word2vec.load_word2vec_format('C:\\Career\\LighthouseLabs\\DataScience_Week10\\GoogleNews-vectors-negative300.bin.gz', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "model = KeyedVectors.load_word2vec_format('C:\\Career\\LighthouseLabs\\DataScience_Week10\\GoogleNews-vectors-negative300.bin.gz', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is 'ADHD' in word2vec?\n",
    "vector = model['ADHD']\n",
    "# see the shape of the vector (300,)\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Attention_Deficit_Hyperactivity_Disorder', 0.7400673031806946),\n",
       " ('attention-deficit/hyperactivity_disorder', 0.7395544648170471),\n",
       " ('attention-deficit/hyperactivity_disorder_ADHD', 0.7256953716278076),\n",
       " ('ADHD_symptoms', 0.7192312479019165),\n",
       " ('ADD_ADHD', 0.7176992297172546),\n",
       " ('attention_deficit_hyperactivity', 0.6985771656036377),\n",
       " ('autism_spectrum_disorder', 0.695148229598999),\n",
       " ('bipolar_disorder', 0.6909056305885315),\n",
       " ('Attention_Deficit_Hyperactive_Disorder', 0.6814495921134949),\n",
       " ('disorder_ADHD', 0.6777404546737671)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"ADHD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6116657"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"ADHD\",\"ADD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31839314"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"ADHD\",\"adult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042513125"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"ADHD\",\"men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10139598"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"ADHD\",\"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2935436"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"ADHD\",\"child\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14544268"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"ADHD\",\"boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10127714"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"ADHD\",\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05966403"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"ADHD\",\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.083276495"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"ADHD\",\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# king - queen = man - woman\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('attention-deficit/hyperactivity_disorder_ADHD', 0.6639786958694458),\n",
       " ('Attention-Deficit/Hyperactivity_Disorder', 0.6271021366119385),\n",
       " ('autism_spectrum_disorder', 0.625386655330658),\n",
       " ('attention-deficit/hyperactivity_disorder', 0.6192680597305298),\n",
       " ('epilepsy', 0.6102749705314636),\n",
       " ('ADHD_symptoms', 0.608331024646759),\n",
       " ('ADD_ADHD', 0.6078106760978699),\n",
       " ('stimulant_medications', 0.6029993295669556),\n",
       " ('perinatal_depression', 0.5994408130645752),\n",
       " ('ADHD_medications', 0.5983275175094604)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'ADHD'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing: Build the corpus\n",
    "corpus = adhdw['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into individual words\n",
    "    tokens = simple_preprocess(text, deacc=True)\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the preprocessing function to each document in the corpus\n",
    "corpus = [preprocess_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['like', 'sub', 'active', 'reddit', 'lots', 'subreddits', 'xx', 'variants', 'supportive', 'contentious', 'examples', 'fitness', 'xxfitness', 'keto', 'xxketo', 'plus', 'stuff', 'main', 'adhd', 'subreddit', 'mods', 'won', 'let', 'talk', 'helpful', 'stuff', 'annoying'], ['ve', 'people', 'receptive', 'responsibility', 'shortfallings', 'especially', 'aware', 'going', 'happen', 'apologize', 'sincerely', 'work', 'plan', 'happening', 'future', 'bit', 'harsh', 'think', 'needs', 'said', 'victim', 'add', 'suffer', 'add', 'ownership', 'control', 'add', 'adhd', 'disorder', 'steps', 'manage', 'longer', 'term', 'project', 'work', 'set', 'mini', 'milestones', 'calendar', 'exact', 'minute', 'project', 'day', 'twice', 'day', 'calendar', 'pops', 'reminds', 'need', 'complete', 'day', 'week', 'order', 'stay', 'projects', 'thing', 'home', 'tell', 'alerts', 'day', 'remind', 'things', 'basic', 'things', 'floss', 'brush', 'teeth', 'check', 'trash', 'lock', 'doors', 'tend', 'away', 'difficult', 'time', 'executive', 'function', 'umbrella', 'term', 'working', 'memory', 'problem', 'soliving', 'task', 'flexibility', 'adaptability', 'build', 'safeguards', 'phone', 'alerts', 'wall', 'calendars', 'fridge', 'calendars', 'sure', 'function', 'like', 'adult', 'figure', 'works', 'write', 'lists', 'set', 'reminders', 'create', 'reward', 'takes'], ['thank', 'trying', 'use', 'phone', 'set', 'alerts', 'reminders', 'time', 'large', 'support', 'group', 'things', 'slip', 'mouths', 'try', 'ignore', 'time', 'hits'], ['deleted'], ['sooooo', 'sure', 'told', 'hours', 'release', 'll', 'hours', 'mg', 'med', 'ritalin', 'short', 'life', 'span', 'body', 'clears', 'pretty', 'quickly', 'basically', 'works', 'capsule', 'tiny', 'beads', 'medicine', 'bigger', 'dissolve', 'different', 'rates', 'different', 'thing', 'affect', 'uptake', 'rates', 'idea', 'll', 'small', 'dose', 'couple', 'hours', 'lull', 'middle', 'll', 'hungry', 'forget', 'actually', 'eat', 'usually', 'good', 'hours', 'lull', 'eat', 'snack', 'good', 'hours', 'll', 'late', 'date', 'fast', 'acting', 'pill', 'help', 'rest', 'day', 'mg', 'lasts', 'hours'], ['doctor', 'reluctant', 'fast', 'acting', 'dosage', 'later', 'afternoon'], ['new', 'doctor', 'know', 'sounds', 'like', 'doctor', 'shopping', 'guess', 'sort', 'think', 'important', 'doctor', 'partner', 'treatment', 'dictator'], ['feel', 'page'], ['good', 'start', 'researching', 'docs', 'listen', 'receptive', 'isn', 'size', 'fits', 'approach', 'treating', 'adhd', 'prescribing', 'dosages', 'doc', 'specializes', 'treating', 'adult', 'adhd', 'psychiatrist', 'talk', 'therapy', 'advice', 'nice', 'listen', 'understands', 'pretty', 'receptive', 'increasing', 'meds', 'felt', 'like', 'dose', 'low'], ['think', 'people', 'realize', 'differently', 'adhd', 'affect', 'gender', 'reading', 'helpful', 'understanding', 'accepting', 'experience', 'adhd', 'thought', 'share', 'http', 'toast', 'net', 'adhd', 'mpdg'], ['yes', 'thank', 'sharing'], ['removed'], ['concerta', 'works', 'tried', 'focalin', 'ritalin', 'switched', 'despite', 'cost', 'years'], ['helpful', 'like', 'adderall', 'better', 'metidate', 'extended', 'release', 'ritalin', 'try', 'instead', 'wellbutrin', 'problems', 'having', 'adderall'], ['anxiety', 'gets', 'raised'], ['yikes', 'fun', 'cut', 'caffeine', 'antacids', 'stuff', 'like', 'try', 'reducing', 'dose', 'saying', 'switch', 'easier', 'changes', 'switch', 'altogether'], ['thanks', 'suggestion'], ['sorry', 'hope', 'come', 'like', 'glossing', 'experience', 'wasn', 'intention', 'sincerely', 'apologize', 'flippant', 'know', 'finding', 'right', 'med', 'dose', 'challenge', 'trying', 'help', 'line', 'dose', 'isn', 'working', 'best', 'talk', 'doc', 'lot', 'options', 'experience', 'insurance', 'pain', 'ass', 'switch', 'medications', 'offering', 'advice', 'mind', 'save', 'headache', 'prior', 'auths', 'anxiety', 'extremely', 'stressful', 'hope', 'able', 'sorted', 'soon', 'relief', 'wishing', 'best'], ['thank'], ['survey', 'closed', 'thank', 'participation', 'expect', 'receive', 'compensation', 'days', 'thank'], ['strong', 'opinion', 'medication', 'questions', 'psychiatrist', 'significant', 'prior', 'experience', 'add', 'makes', 'feel', 'listened', 'respected', 'flexible', 'resourceful', 'trying', 'different', 'meds', 'combinations', 'meds', 'dosages', 'timing', 'committed', 'finding', 'best', 'treatment'], ['medication', 'real', 'grouchy', 'bite', 'heads', 'pms', 'period', 'medication', 'grouchy', 'close', 'time', 'month', 'general', 'lot', 'things', 'irritate', 'irritating', 'anymore'], ['yay', 'nice', 'effect'], ['sounds', 'like', 'female', 'adult', 'adhd', 'misdiagnosed', 'anxiety', 'teachers', 'suggest', 'hs', 'grades', 'good', 'wasn', 'physically', 'hyperactive', 'assumed', 'couldn', 'people', 'doctors', 'realize', 'women', 'slightly', 'different', 'symptoms', 'usually', 'attention', 'deficit', 'hyperactive', 'hyperactivity', 'manifest', 'fidgeting', 'tapping', 'shaking', 'foot', 'chewing', 'nails', 'adhd', 'went', 'long', 'treatment', 'developed', 'anxiety', 'got', 'older', 'anxious', 'symptoms', 'hyper', 'aware', 'anxiety', 'worse', 'having', 'trouble', 'expressing', 'thoughts', 'verbally', 'forgetting', 'talking', 'coming', 'mouth', 'brain', 'tangent', 'idea', 'quickly', 'mouth', 'couldn', 'thinking', 'people', 'conversations', 'try', 'hard', 'pay', 'attention', 'saying', 'end', 'asking', 'repeat', 'embarrassing', 'number', 'times', 'like', 'friend', 'self', 'conscious', 'terrible', 'socializing', 'conversation', 'skills', 'sort', 'retreated', 'got', 'quiet', 'anxiety', 'stutter', 'switch', 'syllables', 'terrible', 'brain', 'fog', 'combination', 'anxiety', 'adhd', 'completing', 'small', 'tasks', 'overwhelming', 'oh', 'mental', 'gymnastics', 'exhausted', 'constantly', 'suggest', 'friend', 'sees', 'psychiatrist', 'specializes', 'adhd', 'women', 'mental', 'health', 'lucky', 'specialist', 'recognized', 'symptoms', 'immediately', 'lot', 'women', 'aren', 'lucky'], ['feedback', 'amazing', 'sounds', 'similar', 'thank', 'sharing', 'talk', 'pulling', 'articles', 'subject', 'appropriate', 'going', 'good', 'start', 'worried', 'going', 'upset', 'like', 'telling', 'mental', 'know', 'promise', 'future', 'like', 'tell', 'knows', 'struggling', 'everyday', 'tasks', 'way', 'like', 'lived', 'life', 'like', 'normal'], ['feedback', 'amazing', 'sounds', 'similar', 'thank', 'sharing', 'talk', 'pulling', 'articles', 'subject', 'appropriate', 'going', 'good', 'start', 'worried', 'going', 'upset', 'like', 'telling', 'mental', 'know', 'promise', 'future', 'like', 'tell', 'knows', 'struggling', 'everyday', 'tasks', 'way', 'like', 'lived', 'life', 'like', 'normal'], ['sure', 'totally', 'aware', 'struggling', 'like', 'll', 'probably', 'relieved', 'likely', 'cause', 'symptoms', 'honestly', 'tell', 'best', 'way', 'approach', 'know', 'person', 'wouldn', 'afraid', 'bring', 'subject'], ['thank', 'help', 'appreciate', 'glad', 'answers'], ['took', 'concerta', 'middle', 'school', 'high', 'school', 'little', 'college', 'switched', 'adderall', 'extended', 'release', 'later', 'difference', 'little', 'agitated', 'adderall', 'fine'], ['explain', 'mean', 'agitated'], ['easy', 'going', 'person', 'lot', 'patience', 'took', 'adderall', 'angry', 'lot', 'easily', 'slightest', 'things', 'couple', 'months'], ['good', 'know'], ['heated', 'concerta', 'tried', 'dexadrine', 'adderall', 'xr', 'love'], ['concerta', 'feel', 'bit', 'impulsive', 'edge', 'short', 'time', 've', 'adderrall', 'xr', 'years', 'years', 'love', 'smooth', 'thankfully', 'calming', 'higher', 'doses', 'mg', 'years', 'bit', 'quiet', 've', 'taking', 'mg', 'depending', 'day', 'personality', 'comes', 'level', 'calm', 'productivity', 'motivation', 'slightly', 'automatic', 'better', 'mg', 'mg', 'wonderful', 'currently', 'life', 'helpful', 'adhd', 'cousins', 'believe', 'took', 'dexedrine', 'think', 'male', 'fared', 'better', 'female', 'contributed', 'night', 'terrors', 'different', 'thankfully', 'stimulants', 'know', 'quickly', 'work', 'searching', 'til', 'happy', 'productive', 'calm', 'best', 'luck'], ['ve', 'tried', 'ritalin', 'la', 'extended', 'release', 'methylphenidate', 'okay', 'ir', 'version', 'great', 'seconds', 'came', 'crashing', 'real', 'hard', 'started', 'lowish', 'dose', 'adderall', 'xr', 'today', 'particularly', 'effective', 'doctor', 'going', 'titrate', 'upwards', 'needed', 'pretty', 'normal', 'adjust', 'dose', 'med', 'periodically'], ['deleted'], ['diagnosis', 'process', 'took', 'visit', 'honestly', 'sending', 'psychiatrist', 'probably', 'experience', 'dealing', 'adhd', 'wouldn', 'worry', 'rambling', 'therapist', 'hilarious', 'random', 'sessions', 'totally', 'normal', 'feel', 'good', 'seeing', 'psychiatrist'], ['seeing', 'psychologist', 'years', 'session', 'suggested', 'adhd', 'like', 'fuck', 'read', 'holy', 'shit', 'life', 'psychiatrist', 'diagnosis', 'shit', 'meds', 'point', 'know', 'fucking', 'loon', 'ace'], ['australia', 've', 'realized', 'adhd', 'teen', 'mother', 'currently', 'single', 'kids', 'different', 'dads', 'live', 'commission', 'house', 'low', 'socioeconomic', 'area', 'terrified', 'dismissed', 'factors', 'contributing', 'belief', 'speed', 'pills', 'way', 'low', 'socioeconomic', 'area', 'wanted', 'speed', 'easier', 'phone', 'calls', 'speed', 'life', 'time', 'suffering', 'symptoms', 'pay', 'hundreds', 'dollars', 'specialist', 'hoping', 'makes', 'sense', 'advice', 'scary', 'stuff', 'compiling', 'list', 'symptoms', 'affect', 'everyday', 'life', 'afford', 'sessions', 'need', 'prepared', 'going', 'ask', 'mum', 'old', 'school', 'reports', 'happening', 'life', 'good', 'luck', 'hope', 'help', 'need'], ['good', 'luck', 'friendly', 'low', 'money', 'appointment', 'gonna', 'set', 'shocked'], ['adhd', 'use', 'vyvanse', 'slow', 'release', 'adderall', 'sort', 'term', 'coverage', 'vyvanse', 'waned', 'round', 'work', 'adderall', 'gave', 'harried', 'jittery', 'feeling', 'caused', 'acne', 'cystic', 'clarity', 'needed', 'hours', 'night', 'low', 'buzz', 'wouldn', 'let', 'sleep', 'basically', 'wasn', 'fit', 'wish', 'knew', 'adderall', 'cause', 'acne', 'wasted', 'tons', 'dermatologist', 'visits', 'amp', 'anti', 'acne', 'products', 'amp', 'rx', 'creams', 'years', 'fyi', 'seeing', 'skin', 'psychiatrist', 'dermatologist', 'gave', 'clue', 'meds', 'root', 'skin', 'problem'], ['adderall', 'xr', 'build', 'days', 'point', 'barely', 'sleep', 'weird', 'scary', 'dreams', 'like', 'having', 'regular', 'adderall', 'right', 'split', 'doses', 'trying', 'figure', 'doctor', 'keeps', 'suggesting', 'trying', 'ssri', 'terrified', 'trying', 'impulsive', 'work', 'progress'], ['wow', 'shoot', 'know', 'glad', 'read', 'comment', 'pretty', 'skincare', 'started', 'adderall', 'xr', 'week', 'ago', 've', 'bewildered', 'cysts', 'coming', 'thank', 'mentioning'], ['sure', 'glad', 'helped'], ['try', 'tracker', 'built', 'options', 'meds', 'option', 'add', 'notes', 'day', 'built', 'options', 'tracking', 'cycle', 'flow', 'cravings', 'intimacy', 'moods', 'add', 'custom', 'moods', 'list', 'presets', 'maybe', 'program', 'meds', 'love', 'data', 'fond', 'app', 'facilitate', 'use', 'notes', 'section', 'tracking', 'things', 'want', 'presets', 'exercise', 'energy', 'level', 'meds', 've', 'developed', 'simple', 'symbol', 'key', 'times', 'focus', 'remember', 'lol', 'enter', 'info', 'order', 'different', 'text', 'lines', 'essentially', 'like', 'template', 'sure', 'consider', 'manual', 'entry', 'know', 'presets', 'looking', 've', 'work', 'free', 'google', 'play', 'store', 'worth', 'try', 'good', 'luck'], ['need', 'record', 'meds', 'need', 'app', 'remind', 'proper', 'times', 'looking', 'tracker', 'remind', 'medications', 'log', 'daily', 'data', 'predictions', 'send', 'notifications', 'allow', 'custom', 'reminders', 'based', 'day', 'number', 'allow', 'create', 'medication', 'regiments', 'based', 'cycle', 'pretty', 'sure', 'app', 'like', 'exist', 've', 'toying', 'idea', 'making'], ['ahh', 'sorry', 'misread', 'misunderstood', 'know', 'thing', 'sounds', 'highly', 'individualized', 'like', 'personal', 'assist', 'trained', 'specifically', 'medication', 'data', 'logging', 'reminders', 'daily', 'repeating', 'alarms', 'google', 'calendar', 'alerts', 'assuming', 'need', 'time', 'day', 'care'], ['thanks', 'ended', 'able', 'strattera', 'based', 'gene', 'test', 'hopefully', 'works'], ['sounds', 'like', 'mypill', 'looking', 'allows', 'customize', 'pill', 'pack', 'set', 'time', 'specific', 'notifications', 'reminders', 'right', 'away', 'work', 'bc', 'types', 'medications'], ['think', 'stuck', 'wasn', 'working', 'long', 'blamed', 'working', 'like', 'wasn', 'disciplined', 'needed', 'different', 'medication', 'oddly', 'taking', 'concerta', 'mg', 'supplement', 'ritalin', 'mg', 'equals', 'concerta', 'mg', 'occasion', 'afternoons', 'extend', 'effects', 'ran', 'concerta', 'day', 'started', 'taking', 'short', 'acting', 'ritalin', 'waited', 'refill', 'took', 'day', 'concerta', 'ritalin', 'supposed', 'drug', 'long', 'acting', 'short', 'acting', 'let', 'tell', 'respond', 'differently', 'concerta', 'anxiety', 'jittery', 'hyperfocus', 'irritable', 'heart', 'pounding', 'motivation', 'help', 'ritalin', 'short', 'acting', 'calm', 'clear', 'motivation', 'increased', 'anxious', 'noticed', 'ritalin', 'took', 'concerta', 'concerta', 'effects', 'present', 'technically', 'wore', 'sort', 'colored', 'experience', 'ritalin'], ['point', 'directly', 'relevant', 'think', 'need', 'tell', 'guys', 'things', 'order', 'know', 'know'], ['agreed', 'adhd', 'dating', 'website', 'exist', 'probably', 'point', 'saying', 'date', 'continues', 'depends', 'comfort', 'level', 'person'], ['lol', 'place'], ['expected', 'adhd', 'person', 'lol'], ['funny', 'reminds', 'lol'], ['personally', 'avoid', 'social', 'interaction', 'possible'], ['basically', 'root', 'depression'], ['dude'], ['somewhat', 'lol'], ['help', 'labeled', 'gifted', 'long', 'acquiring', 'adhd', 'label', 'smart', 'live', 'potential', 'hearing', 'course', 'childhood', 'early', 'adulthood', 'unable', 'articulate', 'brain', 'strange', 'place', 'live', 'bullying', 'social', 'ineptitude', 'general', 'angst', 'woman', 'contemporary', 'society', 'yeah', 'pretty', 'sure', 'recipe', 'depression', 'anxiety'], ['freakin', 'sucks', 've', 'called', 'smart', 'gifted', 'socially', 'awkward', 'conversation', 'interesting', 'thoughts', 'getting', 'way', 'amp', 'irks', 'having', 'good', 'day', 'amp', 'end', 'impressive', 'amp', 'goes', 'wow', 'lily', 'smart', 'guess', 'going', 'brain', 'makes', 'want', 'strangle', 've', 'self', 'conscious', 'intelligence', 'life'], ['adhd', 'diagnosis'], ['thinking', 'getting', 'meds'], ['useful', 'feeling', 'stupid', 'thing', 'wasn', 'diagnosed', 'childhood', 'pretty', 'obviously', 'affected', 'meds', 'times', 'life', 'generally', 'better', 'notion', 'medicating', 'adders', 'makes', 'zombies', 'feel', 'sharp', 'somewhat', 'control', 'medicated', 'effects', 'aren', 'typically', 'associated', 'zombies'], ['yeah', 'remember', 'feeling', 'great', 'meds', 'family', 'tell', 'need', 'head', 'process', 'amp', 'ended', 'having', 'ride', 'psychiatrist', 'appointments', 'happened', 'twice', 'eventually', 'thought', 'maybe', 'sign', 'maybe', 'amp', 'meds', 'aren', 'meant', 'amp', 'told', 'completely', 'fine', 'day', 'reminded', 'different', 'lol', 'feel', 'like', 'takes', 'seriously', 'trusted', 'drive', 'trusted', 'cooking', 'mom', 'thinks', 'll', 'burn', 'house', 'lol', 'people', 'think', 'fuck', 'making', 'money', 'amp', 'drive', 'appointments', 'think', 'time'], ['know', 'feels', 'live', 'inside', 'mind', 'far', 'family'], ['thanks', 'needed', 'encouragement'], ['similar', 'telling', 'date', 'depression', 'bipolar', 'ptsd', 'idea', 'think', 'personality', 'reveal', 'people', 've', 'got', 'staying', 'power'], ['legal', 'ship', 'canada'], ['ad', 'hd', 'relief', 'finally', 'figure', 'felt', 'different', 'stupid', 'way', 'impulsive', 'people', 'ihad', 'classic', 'symptoms', 'total', 'fuck', 'school', 'dropped', 'holy', 'hell', 'horrible', 'life', 'choices', 'totally', 'embraced', 'ad', 'hd', 'meds', 'life', 'better', 'use', 'advantage', 'like', 'makes', 'creative', 'think', 'outside', 'box', 'act', 'outside', 'box', 'teach', 'fitness', 'classes', 'helps', 'focus', 'decided', 'look', 'ad', 'hd', 'super', 'power', 'accept', 'brain', 'let', 'people', 'feel', 'like', 'idiot', 'choose', 'want', 'feel', 'try', 'block', 'opinions', 'suffered', 'years', 'thinking', 'disaster', 'wasn', 'disaster', 'way', 'different', 'way', 'thinking', 'literally', 'moving'], ['fuck', 'brain', 'way', 'rockin', 'people', 'brains', 'try', 'meds', 'got', 'loose', 'sounds', 'like', 'great', 'progress', 'making', 'money', 'drive', 'appointments', 'live', 'ad', 'hd', 'add', 'world', 'thrive'], ['thank'], ['inspiring', 'need', 'work', 'self', 'esteem', 'sure', 'pretty', 'positive'], ['thought', 'sinuses', 'draining', 'lot', 'taking', 'adderall', 'vasodilator', 'opens', 'veins', 'pumps', 'lot', 'blood', 'inflammation', 'sinuses', 'help', 'cause', 'drain', 'vyvanse', 'bad', 'sinus', 'problems', 'severe', 'sinus', 'headaches', 'vyvanse', 'helps', 'allergy', 'meds', 'having', 'reaction', 'ingredient', 'generic', 'depends', 'sinus', 'problem', 'specific'], ['thanks', 'info', 'sorry', 'hear', 'sinus', 'problems', 'glad', 'relief', 'fully', 'empathize', 'nightmare', 'sinus', 'issues', 'new', 'rx', 'real', 'adderall', 'goes', 'thanks'], ['drink', 'tons', 'water', 'dry', 'mouth', 'rinses', 'gum', 'lozenges', 'toothpaste', 'aisle', 'help', 'bad', 'dry', 'mouth', 'helped', 'vyvanse', 'dehydrate', 'lot', 'water', 'essential', 'latest', 'weeks', 'went', 'away'], ['ooh', 'good', 'know', 'lasted', 'weeks', 'went', 'away', 'think', 'body', 'adjusted', 'got', 'better', 'staying', 'hydrated'], ['probably', 'noticed'], ['yeah', 'agree', 'probably', 'notice', 'morning', 'bit', 'drinking', 'couple', 'bottles', 'water', 'gone'], ['fuck', 'struggle', 'drink', 'water', 'maybe', 'ml', 'day', 'joke', 'feeling', 'vyvanse', 'guess', 'new', 'meds', 'new', 'gotta', 'step', 'water'], ['thanks', 'guess', 'probably', 'got', 'better', 'managing', 'realising', 'ok', 'thanks'], ['struggled', 'maintaining', 'consistent', 'journal', 'planner', 'current', 'fantastic', 'little', 'bound', 'daily', 'reminder', 'diary', 'glance', 'https', 'www', 'amazon', 'com', 'glance', 'standard', 'diary', 'inches', 'sd', 'dp', 'tgin', 'ref', 'dp_ob_title_ce', 'like', 'page', 'day', 'giving', 'space', 'write', 'page', 'helpfully', 'marked', 'days', 'lapsed', 'days', 'remaining', 'year', 'ex', 'rd', 'day', 'days', 'follow', 'providing', 'proper', 'sense', 'progression', 'thing', 'like', 'year', 'mini', 'calendar', 'form', 'giving', 'good', 'bird', 'eye', 'view', 'year', 've', 'stuck', 'beginning', 'year', 'recommend', 'try'], ['hello', 'yo', 'diagnosed', 'like', 'months', 'ago', 'life', 'parents', 'shouted', 'told', 'lazy', 'told', 'loosing', 'things', 'late', 'finished', 'studies', 'felt', 'useless', 'self', 'stem', 'society', 'obviously', 'helped', 'helped', 'spend', 'money', 'things', 'need', 'couldn', 'afford', 'thinking', 'years', 'main', 'goal', 'pretty', 'feel', 'pretty', 'stress', 'good', 'getting', 'diagnose', 'finally', 'words', 'wrong', 'different', 'understand', 'love', 'comprehension', 'relationship', 'dad', 'better', 'edit', 'main', 'symptoms', 'forgetfulness', 'clumsiness', 'overload', 'senses', 'experience', 'emotions', 'intensely', 'able', 'concentrate', 'time', 'experiencing', 'hyper', 'focus', 'certain', 'things', 'certain', 'times', 'having', 'difficulties', 'connecting', 'people', 'executive', 'dysfunction', 'anxiety'], ['deleted'], ['thanks', 'sharing', 'wonderful', 'insight', 'glad', 'things', 'mend', 'mind', 'expanding', 'elaborate', 'difficulties', 'connecting', 'people', 'edit', 'thanks', 'sharing', 'resonate', 'eagerness', 'friendship', 'constant', 'need', 'like', 'similar', 'personalities', 'calmed', 'lot', 've', 'moved', 'bigger', 'city', 'friends', 'start', 'busy'], ['ll', 'start', 'saying', 'having', 'adhd', 'problems', 'won', 'details', 'belong', 'justnofamily', 'belong', 'mid', 'thirties', 'early', 'school', 'life', 'occurred', 'internet', 'access', 'hometown', 'lots', 'problems', 'grades', 'll', 'sum', 'saying', 'catholic', 'school', 'went', 'left', 'lot', 'desired', 've', 'pretty', 'dim', 'view', 'certain', 'types', 'religious', 'people', 'incidents', 'bullying', 'abuse', 'total', 'refusal', 'set', 'foot', 'catholic', 'school', 'transferred', 'elementary', 'school', 'hometown', 'public', 'school', 'believe', 'school', 'better', 'setup', 'adhd', 'student', 'actually', 'thrived', 'reasons', 'include', 'lack', 'desks', 'use', 'cubbyholes', 'tables', 'attitudes', 'teachers', 'encouragements', 'interests', 'students', 'ones', 'taught', 'joy', 'reading', 'huge', 'influence', 'later', 'life', 'grades', 'grades', 'high', 'school', 'home', 'town', 'junior', 'high', 'problems', 'll', 'honest', 'point', 'think', 'worth', 'unexpected', 'remark', 'teachers', 'tail', 'end', 'grade', 'changed', 'perspective', 'teacher', 'said', 'moment', 'supreme', 'frustration', 'jarred', 'belief', 'mediocrity', 'teacher', 'said', 'smart', 'need', 'focus', 'honestly', 'moment', 'said', 'smart', 'face', 'heard', 'conviction', 'wrong', 'having', 'bit', 'patronization', 'jarring', 'positive', 'way', 'grades', 'actually', 'managed', 'pull', 'decent', 'marks', 'helped', 'class', 'schedule', 'years', 'classes', 'scheduled', 'time', 'slot', 'day', 'entire', 'semester', 'lot', 'harder', 'forget', 'assignments', 'reminder', 'day', 'chose', 'university', 'want', 'financial', 'burden', 'family', 'lot', 'gaslighting', 'mother', 'father', 'willing', 'able', 'help', 'point', 'wanted', 'toxic', 'dump', 'parents', 'house', 'quickly', 'far', 'moved', 'hours', 'away', 'different', 'province', 'moved', 'wasn', 'good', 'financial', 'decision', 'moved', 'summer', 'heck', 'lot', 'psychological', 'sense', 'stay', 'far', 'away', 'college', 'unexpectedly', 'difficult', 'chance', 'encounter', 'lead', 'meeting', 'diagnosis', 'adhd', 'lot', 'happened', 'll', 'leave', 'time'], ['early', 'life', 'blatantly', 'obvious', 'parents', 'teachers', 'adhd', 'evidence', 'prevailing', 'wisdom', 'boys', 'got', 'adhd', 'girls', 'got', 'tested', 'contrast', 'little', 'brother', 'diagnosed', 'grade', 'gender', 'problems', 'identified', 'adhd', 'early', 'problem', 'interests', 'pulled', 'far', 'away', 'traditional', 'gender', 'roles', 'catholic', 'school', 'meant', 'ridicule', 'bullying', 'public', 'school', 'matter', 'teachers', 'difficult', 'relate', 'girls', 'hometown', 'conservative', 'majority', 'boys', 'friendship', 'girls', 'awesome', 'exceptions', 'college', 'easier', 'city', 'gender', 'roles', 'significantly', 'blurred', 'guys', 'went', 'school', 'care', 'gender', 'long', 'held', 'similar', 'college', 'recognized', 'diagnosed', 'classes', 'heavily', 'dominated', 'men', 'women', 'pretty', 'case', 'class', 'workplace', 've', 'run', 'gender', 'bias', 'issues', 'woman', 'man', 'makes', 'assumption', 'capabilities', 'based', 'solely', 'gender', 'usually', 'long', 'shake', 'beliefs', 've', 'changed', 'people', 'worldviews', 'simply', 'ones', 'haven', 'changed', 'generally', 'ones', 'weren', 'worth', 'difficult', 'yes', 've', 'outsider', 'life', 'tough', 'fringe', 'easier', 'easier', 'accepted', 'unconventional', 'live', 'celebrated', 'certain', 'extent', 'struggle', 'stay', 'task', 'medication', 'helps', 'best', 'thing', 'accept', 'unique', 'limitations', 'work', 'bypass', 'limitations', 'methods', 'prove', 'effective'], ['got', 'suggested', 'adhd', 've', 'formally', 'diagnosed', 'knows', 'obvious', 'started', 'medication', 'anxiety', 'feel', 'day', 'work', 'related', 'fear', 'tasks', 'time', 'organised', 'making', 'good', 'impact', 'having', 'ability', 'job', 'overwhelmed', 'certain', 'tasks', 'like', 'cleaning', 'room', 'deal', 'partner', 'stepping', 'write', 'lists', 'sure', 'use', 'pre', 'paid', 'card', 'help', 'budget', 'try', 'hard', 'emotions', 'check', 'reactions', 'stuff', 'good', 'thing', 'hyper', 'focusing'], ['sure', 'mind', 'ask', 'want', 'sociable', 'need', 'human', 'connection', 'spend', 'days', 'seeing', 'people', 'feel', 'like', 'friends', 'depressed', 'impatient', 'tent', 'force', 'friendships', 'little', 'bit', 'instead', 'getting', 'know', 'slowly', 'usually', 'excited', 'fast', 'person', 'spoil', 'expecting', 'quickly', 'frustrated', 'feel', 'like', 'won', 'able', 'friends', 'anymore', 'thing', 'help', 'group', 'people', 'hard', 'follow', 'talking', 'usually', 'lost', 'feel', 'completely', 'place', 'things', 'think', 'far'], ['thank', 'sharing', 'wow', 'pretty', 'par', 'problems', 'hope', 've', 'managing', 'best', 'luck'], ['thanks', 'sharing', 'meant', 'interaction', 'gender', 'symptoms', 'experience', 'assumed', 'juggle', 'tasks', 'wear', 'hats', 'women', 'supposed', 'good', 'multitasking', 'issue', 'function', 'stimulus', 'got', 'reassurance', 'women', 'succeeded', 'got', 'feel', 'like', 'par', 'peers', 'accomplish', 'lead', 'diagnosis', 'general', 'anxiety', 'disorder', 'depressive', 'symptoms', 'wonder', 'deeper', 'issue', 'anxiety', 'depression', 'symptoms', 'bigger', 'issue'], ['thank', 'sharing', 'glad', 'hear', 've', 'gotten', 'toxic', 'environment', 'hope'], ['honestly', 'life', 'felt', 'different', 'amp', 'know', 'frustrating', 'ended', 'pointing', 'fact', 'maybe', 'stupid', 'couldn', 'simple', 'things', 'like', 'pay', 'attention', 'class', 'little', 'world', 'got', 'diagnosed', 'adhd', 'going', 'psychiatrist', 'specifically', 'depression', 'lol', 'research', 'adhd', 'trying', 'wrong', 'point', 'diagnosed', 'like', 'knew', 'lol'], ['thanks', 'sharing', 'mind', 'sharing', 'treatments'], ['mess', 'twice', 'family', 'members', 'discouraged', 'yanno', 'adhd', 'isn', 'real', 'lazy', 'yeah', 'lol', 'follow', 'appts', 'seriously', 'late', 'amp', 'stuff', 'ended', 'giving', 'shrugs', 'seriously', 'thinking', 'going', 'meds', 'able'], ['ha', 'according', 'studies', 've', 'read', 'good', 'multitasking', 'conclusions', 'avoid', 'multitasking', 'costs', 'empirical', 'evidence', 'shown', 'time', 'penalty', 'task', 'switch', 'person', 'efficiency', 'decreased', 'makes', 'different', 'level', 'control', 'frustration', 'reduced', 'executive', 'function', 'capacity', 'task', 'switching', 'infinitely', 'harder', 'likely', 'll', 'frustrated', 'upset', 'focus', 'disrupted', 'add', 'lack', 'interested', 'subject', 've', 'forced', 'switch', 've', 'got', 'recipe', 'problems', 'thing', 'consider', 'aware', 'stimulus', 'great', 'difficulty', 'tuning', 'noise', 'medication', 'helped', 'lot', 'strategies', 'help', 'including', 'wearing', 'earplugs', 'noise', 'cancelling', 'headphones', 'wearing', 'comfortable', 'clothes', 'working', 'quiet', 'room', 'work', 'walking', 'treadmill', 'standing', 'desk', 'reasonable', 'accommodations', 'employer'], ['certainly', 'cognitive', 'processing', 'slow', 'screeching', 'halt', 'time', 'feels', 'like', 'body', 'experience', 'connect', 'outside', 'world', 'isn', 'experience', 'anymore', 'thank', 'goodness', 'drugs', 'self', 'care', 'techniques', 'happens'], ['meds', 'help', 'mean', 'self', 'care', 'techniques'], ['yes', 'medications', 'easier', 'maintain', 'focus', 'allocate', 'energy', 'thinking', 'spending', 'energy', 'trying', 'pay', 'attention', 'longer', 'energy', 'think', 'hit', 'wall', 'speak', 'self', 'care', 'technique', 'mean', 'getting', 'sleep', 'hours', 'eating', 'exercising', 'aren', 'taking', 'care', 'energy', 'things', 'need'], ['think', 'getting', 'sleep', 'definitely', 'exercise', 'amp', 'getting', 'habit', 'eating', 'better', 'guess', 'half', 'way', 'mind', 'ask', 'kind', 'meds', 'taking']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaing the text (if needed)\n",
    "# processed_corpus = article_text.lower()\n",
    "# processed_corpus = re.sub('[^a-zA-Z]', ' ', processed_corpus)\n",
    "# processed_corpus = re.sub(r'\\s+', ' ', processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      3\u001b[0m \u001b[39m# Train the Word2Vec model on the preprocessed corpus\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(corpus, size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, window\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, min_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the Word2Vec model on the preprocessed corpus\n",
    "model = Word2Vec(corpus, size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adhdw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparsing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m STOPWORDS\n\u001b[0;32m      5\u001b[0m \u001b[39m# Step 1: Extract the text from the 'adhdw['body']' column and store it as a list\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m corpus \u001b[39m=\u001b[39m adhdw[\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m      8\u001b[0m \u001b[39m# Step 2: Preprocess the text by tokenizing and cleaning it\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(text):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adhdw' is not defined"
     ]
    }
   ],
   "source": [
    "# size now vector_size in w2v arguments\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# Step 1: Extract the text from the 'adhdw['body']' column and store it as a list\n",
    "corpus = adhdw['body'].tolist()\n",
    "\n",
    "# Step 2: Preprocess the text by tokenizing and cleaning it\n",
    "def preprocess_text(text):\n",
    "    tokens = simple_preprocess(text, deacc=True)\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# Step 3: Train the Word2Vec model on the preprocessed corpus\n",
    "model = Word2Vec(corpus, size=100, window=5, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Kaggle tutorial https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial\n",
    "# w2v_model = Word2Vec(min_count=20,\n",
    "#                      window=2,\n",
    "#                      vector_size=300,\n",
    "#                      sample=6e-5, \n",
    "#                      alpha=0.03, \n",
    "#                      min_alpha=0.0007, \n",
    "#                      negative=20,\n",
    "#                      workers=cores-1)\n",
    "ww2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thisone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
